12个类，每个种类有不同的生长阶段

分别使用特征工程（特征提取算法），和深度学习算法进行模型构建和对比

利用opencv库中特征提取的算法，如：HOG，sift（bag of word）等

main2.py打包数据

MachinLearning.ipynb使用HOG、GRAY、LBP、ORB四个特征，使用XGBoost训练模型，得分有0.79093

DeepLearning.ipynb使用resnet50训练，epoch=40，batch_size=64，耗时40min左右，得分有0.80352


机器学习

1. 基础理论（介绍自己使用的方法基本原理）

方向梯度直方图（Histogram of Oriented Gradient, HOG）

快速特征点提取（Oriented FAST and Rotated BRIEF,ORB）

局部二值模式（Local Binary Pattern，LBP）

2. 方法/流程介绍

    1.获取数据
        实验数据从kaggle网站上下载得到，打包成pkl文件传入到程序中。

    2.数据处理
        将得到的图像，通过翻转增强和旋转增强处理图像，从而得到更多的实验数据。

    3.特征提取
        首先对图像进行掩膜（mask）处理，然后用高斯滤波器进行图像的锐化，之后分别用HOG、OBR、LBP、GRAY方法进行特征提取，
        再用preprocessing库中的函数进行特征归一化，最后把归一化得到的特征数据融合到一起。

    4.特征降维
        尝试用线性和非线性的方法进行特征降维，进而缩短训练时间，但得到的准确率不是很理想。

    5.机器学习算法进行训练 - 模型
        我们分别采用了svm、xgboost、随机森林的方法进行训练，最后选择了效果更好的xgboost。

    6.模型评估 （如果评估结果不合适则循环2-5步骤，直到通过第五步）

3. 实验结果与分析

特征处理后得到数据，数据又降维到3维。

用SVM处理未降维数据训练、预测用时21m8s，得分为：0.40869，处理降维的数据训练、预测用时17.13s，得分为：0.40869。
用RandomForest处理未降维数据训练、预测用时1m47s，得分为：0.64861，处理降维的数据训练、预测用时3.7s，得分为：0.41435。
用XGBoost处理未降维数据训练、预测用时25m54s，得分为：0.79093，处理降维的数据训练、预测用时8.8s，得分为：0.40176。

深度学习

1. 基础理论（介绍自己使用的方法基本原理）

使用ResNet50。
Resnet50 网络中包含了 49 个卷积层、一个全连接层，分为七个部分。
I部分不包含残差块，主要对输入进行卷积、正则化、激活函数、最大池化的计算。
II-V部分结构都包含了残差块，CONVBLOCK不会改变残差块的尺寸，只用于改变残差块的维度。在 Resnet50 网 络 结 构 中 ， 残 差 块 都 有 三 层 卷 积 ， 那 网 络 总 共 有1+3×（3+4+6+3）=49个卷积层，加上最后的全连接层总共是 50 层。网络的输入为 224×224×3，经过前五部分的卷积计算，输出为 7×7×2048，池化层会将其转化成一个特征向量，最后分类器会对这个特征向量进行计算并输出类别概率。

2. 方法/流程介绍

    1.创建验证集（20%）训练集（80%）后，数据处理归一化。

    2.使用ReduceLROnPlateau策略（当特定的度量指标，如训练损失、验证损失或准确率不再变化时，学习率就会改变）。

    3.定义损失函数和优化器。

    4.建立ResNet50模型，为了适应自己的数据集,将ResNet-50的最后一层替换为,将原来最后一个全连接层的输入喂给一个有12个输出单元的线性层。

    5.训练40轮。


3. 实验结果与分析

没有调整参数的优化，得分0.80352。