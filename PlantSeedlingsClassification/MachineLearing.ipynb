{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from skimage.feature import local_binary_pattern\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pic_shape = (200, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集长度: 4750 测试集长度: 794\n"
     ]
    }
   ],
   "source": [
    "train_dataset = pickle.load(open('E:/py/MachineLearing/MachineLearning-CourseExercise/my_train.pkl', 'rb'))\n",
    "test_dataset = pickle.load(open('E:/py/MachineLearing/MachineLearning-CourseExercise/my_test.pkl', 'rb'))\n",
    "print('训练集长度:', len(train_dataset['data']), '测试集长度:', len(test_dataset['data']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据拓展\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(train_dataset['data'])):\n",
    "    img=train_dataset['data'][i]\n",
    "    target=train_dataset['target'][i]\n",
    "    # 翻转增强\n",
    "    train_dataset['data'].append(cv2.flip(img, -1))\n",
    "    train_dataset['target'].append(target)\n",
    "    train_dataset['data'].append(cv2.flip(img, 1))\n",
    "    train_dataset['target'].append(target)\n",
    "\n",
    "    # 旋转增强\n",
    "    # getRotationMatrix2D(旋转中心,旋转角度,缩放比例)\n",
    "    M = cv2.getRotationMatrix2D((int(pic_shape[0]*0.5),int(pic_shape[1]*0.5)), 45, 1)\n",
    "    dst = cv2.warpAffine(img, M, pic_shape)\n",
    "    train_dataset['data'].append(dst)\n",
    "    train_dataset['target'].append(target)\n",
    "    M = cv2.getRotationMatrix2D((int(pic_shape[0]*0.5),int(pic_shape[1]*0.5)), 90, 1)\n",
    "    dst = cv2.warpAffine(img, M, pic_shape)\n",
    "    train_dataset['data'].append(dst)\n",
    "    train_dataset['target'].append(target)\n",
    "    M = cv2.getRotationMatrix2D((int(pic_shape[0]*0.5),int(pic_shape[1]*0.5)), 135, 1)\n",
    "    dst = cv2.warpAffine(img, M, pic_shape)\n",
    "    train_dataset['data'].append(dst)\n",
    "    train_dataset['target'].append(target)\n",
    "\n",
    "#\n",
    "print('数据拓展')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_mask_for_plant(image):\n",
    "    image_hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    # 突出绿色部分\n",
    "    sensitivity = 35\n",
    "    lower_hsv = np.array([60 - sensitivity, 100, 50])\n",
    "    upper_hsv = np.array([60 + sensitivity, 255, 255])\n",
    "    # 输出的图片为二值化图只有黑白两种颜色\n",
    "    mask = cv2.inRange(image_hsv, lower_hsv, upper_hsv)\n",
    "\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11, 11))\n",
    "    # 形态学滤波\n",
    "    # cv2.morphologyEx(img, op, kernel)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def segment_plant(image):\n",
    "    mask = create_mask_for_plant(image)\n",
    "    output = cv2.bitwise_and(image, image, mask=mask)\n",
    "    return output\n",
    "\n",
    "\n",
    "def sharpen_image(image):\n",
    "    # cv2.GaussianBlur(SRC,ksize,sigmaX [,DST [,sigmaY [,borderType ] ] ] ) \n",
    "    # 减少噪声\n",
    "    image_blurred = cv2.GaussianBlur(image, (0, 0), 3)\n",
    "    image_sharp = cv2.addWeighted(image, 1.5, image_blurred, -0.5, 0)\n",
    "    return image_sharp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv2.MORPH_OPEN\t开运算(open) ,先腐蚀后膨胀的过程。开运算可以用来消除小黑点，在纤细点处分离物体、平滑较大物体的边界的 同时并不明显改变其面积。\n",
    "# cv2.MORPH_CLOSE\t闭运算(close)，先膨胀后腐蚀的过程。闭运算可以用来排除小黑洞。\n",
    "# cv2.MORPH_GRADIENT\t形态学梯度(morph-grad)，可以突出团块(blob)的边缘，保留物体的边缘轮廓。\n",
    "# cv2.MORPH_TOPHAT\t顶帽(top-hat)，将突出比原轮廓亮的部分。\n",
    "# cv2.MORPH_BLACKHAT\t黑帽(black-hat)，将突出比原轮廓暗的部分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "winSize = pic_shape\n",
    "blockSize = (int(pic_shape[0]*0.2),int(pic_shape[1]*0.2))\n",
    "blockStride = (int(pic_shape[0]*0.2),int(pic_shape[1]*0.2))\n",
    "cellSize = (int(pic_shape[0]*0.1),int(pic_shape[1]*0.1))\n",
    "nbins = 4\n",
    "hog = cv2.HOGDescriptor(winSize, blockSize, blockStride, cellSize, nbins)\n",
    "orb=cv2.ORB_create(nfeatures=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nfeatures ：最多提取的特征点的数量；\n",
    "# scaleFactor ： 金字塔图像之间的尺度参数，类似于SIFT中的k；\n",
    "# nlevels： 高斯金字塔的层数；\n",
    "# edgeThreshold ：边缘阈值，这个值主要是根据后面的patchSize来定的，靠近边缘edgeThreshold以内的像素是不检测特征点的。\n",
    "# firstLevel-：看过SIFT都知道，我们可以指定第一层的索引值，这里默认为0。\n",
    "# WET_K ： 用于产生BIREF描述子的点对的个数，一般为2个，也可以设置为3个或4个，那么这时候描述子之间的距离计算就不能用汉明距离了，而是应该用一个变种。OpenCV中，如果设置WET_K = 2，则选用点对就只有2个点，匹配的时候距离参数选择NORM_HAMMING，如果WET_K设置为3或4，则BIREF描述子会选择3个或4个点，那么后面匹配的时候应该选择的距离参数为NORM_HAMMING2。\n",
    "# scoreType ：用于对特征点进行排序的算法，你可以选择HARRIS_SCORE，也可以选择FAST_SCORE，但是它也只是比前者快一点点而已。\n",
    "# patchSize ：用于计算BIREF描述子的特征点邻域大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28500/28500 [05:07<00:00, 92.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOG特征维度 (3600,)\n",
      "LBP特征维度 (256,)\n",
      "ORB特征维度 (1600,)\n",
      "GRAY特征维度 (400,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 794/794 [00:08<00:00, 89.54it/s]\n"
     ]
    }
   ],
   "source": [
    "winStride = (8, 8)\n",
    "padding = (8, 8)\n",
    "\n",
    "train_HOG_feature=[]\n",
    "train_ORB_feature=[]\n",
    "train_LBP_feature=[]\n",
    "train_GRAY_feature=[]\n",
    "\n",
    "\n",
    "test_HOG_feature=[]\n",
    "test_ORB_feature=[]\n",
    "test_LBP_feature=[]\n",
    "test_GRAY_feature=[]\n",
    "\n",
    "for img_data in tqdm(train_dataset['data']):\n",
    "    image_segmented = segment_plant(img_data)\n",
    "    image_sharpen = sharpen_image(image_segmented)\n",
    "    gray = cv2.cvtColor(image_sharpen, cv2.COLOR_BGR2GRAY)\n",
    "    # 图像数据生成mask和gray\n",
    "    \n",
    "    #resize后的小型图像\n",
    "    gray_resized=cv2.resize(gray, (20, 20))\n",
    "    train_GRAY_feature.append(gray_resized.reshape((-1,)))\n",
    "    \n",
    "    # lbp统计直方图\n",
    "    lbp = local_binary_pattern(gray,P=8,R=3)\n",
    "    max_bins=lbp.max()\n",
    "    lbp_hist,_=np.histogram(lbp.reshape((-1,)), normed=True, density=True, bins=256, range=(0, max_bins))\n",
    "    train_LBP_feature.append(lbp_hist)\n",
    "    \n",
    "    # orb特征\n",
    "    ORB_zero=np.zeros((50,32))\n",
    "    kp1, des1 = orb.detectAndCompute(gray, None)\n",
    "    try:\n",
    "        ORB=np.pad(des1,((0,50-des1.shape[0]),(0,0)),'constant')\n",
    "    except:\n",
    "        ORB=np.zeros((50,32))\n",
    "    assert ORB.shape==(50,32)\n",
    "    train_ORB_feature.append(ORB.reshape((-1,)))\n",
    "    # hog特征\n",
    "    #hog_result = hog.compute(image_sharpen, winStride, padding).reshape((-1,))\n",
    "    hog_result = hog.compute(gray, winStride, padding).reshape((-1,))\n",
    "    train_HOG_feature.append(hog_result)\n",
    "\n",
    "print('HOG特征维度',train_HOG_feature[0].shape)\n",
    "print('LBP特征维度',train_LBP_feature[0].shape)\n",
    "print('ORB特征维度',train_ORB_feature[0].shape)\n",
    "print('GRAY特征维度',train_GRAY_feature[0].shape)\n",
    "\n",
    "\n",
    "for img_data in tqdm(test_dataset['data']):\n",
    "    image_segmented = segment_plant(img_data)\n",
    "    image_sharpen = sharpen_image(image_segmented)\n",
    "    gray = cv2.cvtColor(image_sharpen, cv2.COLOR_BGR2GRAY)\n",
    "    # 图像数据生成mask和gray\n",
    "\n",
    "    #resize后的小型图像\n",
    "    gray_resized=cv2.resize(gray, (20, 20))\n",
    "    test_GRAY_feature.append(gray_resized.reshape((-1,)))\n",
    "    \n",
    "    # lbp统计直方图\n",
    "    lbp = local_binary_pattern(gray,P=8,R=3)\n",
    "    max_bins = lbp.max()\n",
    "    lbp_hist,_ = np.histogram(lbp.reshape((-1,)), normed=True, density=True, bins=256, range=(0, max_bins))\n",
    "    test_LBP_feature.append(lbp_hist)\n",
    "    #\n",
    "    # orb特征\n",
    "\n",
    "    kp1, des1 = orb.detectAndCompute(gray, None)\n",
    "    try:\n",
    "        ORB=np.pad(des1,((0,50-des1.shape[0]),(0,0)),'constant')\n",
    "    except:\n",
    "        ORB=np.zeros((50,32))\n",
    "    assert ORB.shape==(50,32)\n",
    "    test_ORB_feature.append(ORB.reshape((-1,)))\n",
    "    # hog特征\n",
    "    #hog_result = hog.compute(image_sharpen, winStride, padding).reshape((-1,))\n",
    "    hog_result = hog.compute(gray, winStride, padding).reshape((-1,))\n",
    "    test_HOG_feature.append(hog_result)\n",
    "# 特征提取\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征归一化完成\n"
     ]
    }
   ],
   "source": [
    "# 特征归一化\n",
    "\n",
    "train_MinMax_HOG = preprocessing.MinMaxScaler()\n",
    "train_MinMax_HOG_data = train_MinMax_HOG.fit_transform(train_HOG_feature)\n",
    "train_HOG_feature = train_MinMax_HOG.inverse_transform(train_MinMax_HOG_data)\n",
    "\n",
    "train_MinMax_LBP = preprocessing.MinMaxScaler()\n",
    "train_MinMax_LBP_data = train_MinMax_LBP.fit_transform(train_LBP_feature)\n",
    "train_LBP_feature = train_MinMax_LBP.inverse_transform(train_MinMax_LBP_data)\n",
    "\n",
    "train_MinMax_ORB = preprocessing.MinMaxScaler()\n",
    "train_MinMax_ORB_data = train_MinMax_ORB.fit_transform(train_ORB_feature)\n",
    "train_ORB_feature = train_MinMax_ORB.inverse_transform(train_MinMax_ORB_data)\n",
    "\n",
    "train_MinMax_GRAY = preprocessing.MinMaxScaler()\n",
    "train_MinMax_GRAY_data = train_MinMax_GRAY.fit_transform(train_GRAY_feature)\n",
    "train_GRAY_feature = train_MinMax_GRAY.inverse_transform(train_MinMax_GRAY_data)\n",
    "\n",
    "\n",
    "test_MinMax_HOG = preprocessing.MinMaxScaler()\n",
    "test_MinMax_HOG_data = test_MinMax_HOG.fit_transform(test_HOG_feature)\n",
    "test_HOG_feature = test_MinMax_HOG.inverse_transform(test_MinMax_HOG_data)\n",
    "\n",
    "test_MinMax_LBP = preprocessing.MinMaxScaler()\n",
    "test_MinMax_LBP_data = test_MinMax_LBP.fit_transform(test_LBP_feature)\n",
    "test_LBP_feature = test_MinMax_LBP.inverse_transform(test_MinMax_LBP_data)\n",
    "\n",
    "test_MinMax_ORB = preprocessing.MinMaxScaler()\n",
    "test_MinMax_ORB_data = test_MinMax_ORB.fit_transform(test_ORB_feature)\n",
    "test_ORB_feature = test_MinMax_ORB.inverse_transform(test_MinMax_ORB_data)\n",
    "\n",
    "test_MinMax_GRAY = preprocessing.MinMaxScaler()\n",
    "test_MinMax_GRAY_data = test_MinMax_GRAY.fit_transform(test_GRAY_feature)\n",
    "test_GRAY_feature = test_MinMax_GRAY.inverse_transform(test_MinMax_GRAY_data)\n",
    "\n",
    "print('特征归一化完成')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train综合特征维度 (28500, 5856)\n",
      "test综合特征维度 (794, 5856)\n",
      "特征提取结束\n"
     ]
    }
   ],
   "source": [
    "# 特征融合\n",
    "\n",
    "train_feature=np.hstack([np.array(train_HOG_feature),np.array(train_LBP_feature),np.array(train_ORB_feature),np.array(train_GRAY_feature)])\n",
    "test_feature=np.hstack([np.array(test_HOG_feature),np.array(test_LBP_feature),np.array(test_ORB_feature),np.array(test_GRAY_feature)])\n",
    "# train_feature=np.hstack([np.array(train_HOG_feature),np.array(train_LBP_feature),np.array(train_ORB_feature)])\n",
    "# test_feature=np.hstack([np.array(test_HOG_feature),np.array(test_LBP_feature),np.array(test_ORB_feature)])\n",
    "\n",
    "# train_feature=np.array(train_HOG_feature)\n",
    "# test_feature=np.array(test_HOG_feature)\n",
    "print('train融合特征维度', train_feature.shape)\n",
    "print('test融合特征维度', test_feature.shape)\n",
    "print('特征提取结束')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据降维开始\n",
      "(29294, 3)\n"
     ]
    }
   ],
   "source": [
    "# #数据降维\n",
    "# print('数据降维开始')\n",
    "# n_components=2000\n",
    "# train_len=len(train_feature)\n",
    "# data=np.vstack([train_feature,test_feature])\n",
    "\n",
    "# pca_tsne = TSNE(n_components=3)\n",
    "# newData_linear = pca_tsne.fit_transform(data)\n",
    "\n",
    "# # sklearn_kpca = KernelPCA(n_components=n_components, kernel=\"rbf\", gamma=15)\n",
    "# # newData_nonlinear = sklearn_kpca.fit_transform(data)\n",
    "\n",
    "# newData = newData_linear\n",
    "# print(newData.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主成分分析（PCA）是使用线性映射将数据进行降维，但是通常情况下高维到低维是非线性的，往往达不到预期的结果。核主成分分析（KPCA）将原始数据通过选择适当的核函数（Kernel）映射到高维空间，再利用高维度空间进行线性降维，是一种用于非线性分类的降维工具。因此 KPCA的核心就是核函数。同时，KPCA采用了比较复杂的非线性映射，提高了非线性数据的处理效率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 降维后的数据\n",
    "# train_feature_low=newData[0:train_len]\n",
    "# assert train_len==len(train_feature_low)\n",
    "# test_feature_low=newData[train_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征数据存储结束\n"
     ]
    }
   ],
   "source": [
    "# #特征数据存储\n",
    "# train_feature_dist=train_dataset.copy()\n",
    "# train_feature_dist['data']=train_feature\n",
    "# pickle.dump(train_feature_dist,open('E:/py/MachineLearing/MachineLearning-CourseExercise/PlantSeedlingsClassification/train_feature_HOG.pkl','wb'))\n",
    "# test_feature_dist=test_dataset.copy()\n",
    "# test_feature_dist['data']=test_feature\n",
    "# pickle.dump(test_feature_dist,open('E:/py/MachineLearing/MachineLearning-CourseExercise/PlantSeedlingsClassification/test_feature_HOG.pkl','wb'))\n",
    "# print('特征数据存储结束')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM分类开始\n",
      "114514\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # SVM分类(降维)\n",
    "\n",
    "# print('SVM分类开始')\n",
    "\n",
    "# modelSVM = svm.SVC()\n",
    "# modelSVM.fit(train_feature_low, train_dataset['target'])\n",
    "# predictedSVM = modelSVM.predict(test_feature_low)\n",
    "\n",
    "# print('114514\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM分类开始\n",
      "114514\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # SVM分类\n",
    "\n",
    "# print('SVM分类开始')\n",
    "\n",
    "# modelSVM = svm.SVC()\n",
    "# modelSVM.fit(train_feature, train_dataset['target'])\n",
    "# predictedSVM = modelSVM.predict(test_feature)\n",
    "\n",
    "# print('114514\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF分类开始\n",
      "114514\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # 随机森林分类(降维)\n",
    "# print('RF分类开始')\n",
    "\n",
    "# modelRF = RandomForestClassifier()\n",
    "# modelRF.fit(train_feature_low, train_dataset['target'])\n",
    "# predictedRF = modelRF.predict(test_feature_low)\n",
    "\n",
    "# print('114514\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 随机森林分类\n",
    "# print('RF分类开始')\n",
    "\n",
    "# modelRF = RandomForestClassifier()\n",
    "# modelRF.fit(train_feature, train_dataset['target'])\n",
    "# predictedRF = modelRF.predict(test_feature)\n",
    "\n",
    "# print('114514\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgboost分类开始\n",
      "114514\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#XGBoost分类(降维)\n",
    "print('Xgboost分类开始')\n",
    "\n",
    "model = XGBClassifier(max_depth=5)\n",
    "model.fit(train_feature_low, train_dataset['target'])\n",
    "predictedXG = model.predict(test_feature_low)\n",
    "\n",
    "print('114514\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgboost分类开始\n",
      "114514\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#XGBoost分类\n",
    "print('Xgboost分类开始')\n",
    "\n",
    "model = XGBClassifier(max_depth=5)\n",
    "model.fit(train_feature, train_dataset['target'])\n",
    "predictedXG = model.predict(test_feature)\n",
    "\n",
    "print('114514\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def model_kf(my_model):\n",
    "#     kf = KFold(5, shuffle=True, random_state=50).get_n_splits(train_feature)\n",
    "#     result_list= np.sqrt(-cross_val_score(my_model, train_feature, train_dataset['target'], scoring=\"f1\", cv = kf))\n",
    "#     return(result_list)\n",
    "# model_kf(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 结果生成\n",
    "# pred = np.exp(predicted)\n",
    "# print(predicted)\n",
    "# subSVM=pd.read_csv('E:/py/MachineLearing/MachineLearning-CourseExercise/PlantSeedlingsClassification/sample_submission.csv')\n",
    "# subSVM['file'] = test_dataset['file_name']\n",
    "# subSVM['species'] = list(map(lambda x:train_dataset['dict'][x], predictedSVM))\n",
    "# subSVM.to_csv('E:/py/MachineLearing/MachineLearning-CourseExercise/PlantSeedlingsClassification/submission_SVM.csv', index=False)\n",
    "\n",
    "# subRF=pd.read_csv('E:/py/MachineLearing/MachineLearning-CourseExercise/PlantSeedlingsClassification/sample_submission.csv')\n",
    "# subRF['file'] = test_dataset['file_name']\n",
    "# subRF['species'] = list(map(lambda x:train_dataset['dict'][x], predictedRF))\n",
    "# subRF.to_csv('E:/py/MachineLearing/MachineLearning-CourseExercise/PlantSeedlingsClassification/submission_RF.csv', index=False)\n",
    "\n",
    "subXG=pd.read_csv('E:/py/MachineLearing/MachineLearning-CourseExercise/PlantSeedlingsClassification/sample_submission.csv')\n",
    "subXG['file'] = test_dataset['file_name']\n",
    "subXG['species'] = list(map(lambda x:train_dataset['dict'][x], predictedXG))\n",
    "subXG.to_csv('E:/py/MachineLearing/MachineLearning-CourseExercise/PlantSeedlingsClassification/submission_XG.csv', index=False)\n",
    "\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "49844d814d8b3527cdd6e0ee9ead42f5b31a2a1e9678336de9e62eb2a92861cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
