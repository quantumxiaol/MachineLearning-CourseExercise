{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from skimage.feature import local_binary_pattern\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pic_shape = (200, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集长度: 4750 测试集长度: 794\n"
     ]
    }
   ],
   "source": [
    "train_dataset = pickle.load(open('E:/py/MachineLearing/MachineLearning-CourseExercise/my_train.pkl', 'rb'))\n",
    "test_dataset = pickle.load(open('E:/py/MachineLearing/MachineLearning-CourseExercise/my_test.pkl', 'rb'))\n",
    "print('训练集长度:', len(train_dataset['data']), '测试集长度:', len(test_dataset['data']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据拓展完成\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(train_dataset['data'])):\n",
    "    img=train_dataset['data'][i]\n",
    "    target=train_dataset['target'][i]\n",
    "    # 翻转增强\n",
    "    train_dataset['data'].append(cv2.flip(img, -1))\n",
    "    train_dataset['target'].append(target)\n",
    "    train_dataset['data'].append(cv2.flip(img, 1))\n",
    "    train_dataset['target'].append(target)\n",
    "\n",
    "    # 旋转增强\n",
    "    # getRotationMatrix2D(旋转中心,旋转角度,缩放比例)\n",
    "    M = cv2.getRotationMatrix2D((int(pic_shape[0]*0.5),int(pic_shape[1]*0.5)), 45, 1)\n",
    "    dst = cv2.warpAffine(img, M, pic_shape)\n",
    "    train_dataset['data'].append(dst)\n",
    "    train_dataset['target'].append(target)\n",
    "    M = cv2.getRotationMatrix2D((int(pic_shape[0]*0.5),int(pic_shape[1]*0.5)), 90, 1)\n",
    "    dst = cv2.warpAffine(img, M, pic_shape)\n",
    "    train_dataset['data'].append(dst)\n",
    "    train_dataset['target'].append(target)\n",
    "    M = cv2.getRotationMatrix2D((int(pic_shape[0]*0.5),int(pic_shape[1]*0.5)), 135, 1)\n",
    "    dst = cv2.warpAffine(img, M, pic_shape)\n",
    "    train_dataset['data'].append(dst)\n",
    "    train_dataset['target'].append(target)\n",
    "\n",
    "#\n",
    "print('数据拓展完成')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_mask_for_plant(image):\n",
    "    image_hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    sensitivity = 35\n",
    "    lower_hsv = np.array([60 - sensitivity, 100, 50])\n",
    "    upper_hsv = np.array([60 + sensitivity, 255, 255])\n",
    "\n",
    "    mask = cv2.inRange(image_hsv, lower_hsv, upper_hsv)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11, 11))\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def segment_plant(image):\n",
    "    mask = create_mask_for_plant(image)\n",
    "    output = cv2.bitwise_and(image, image, mask=mask)\n",
    "    return output\n",
    "\n",
    "\n",
    "def sharpen_image(image):\n",
    "    image_blurred = cv2.GaussianBlur(image, (0, 0), 3)\n",
    "    image_sharp = cv2.addWeighted(image, 1.5, image_blurred, -0.5, 0)\n",
    "    return image_sharp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "winSize = pic_shape\n",
    "blockSize = (int(pic_shape[0]*0.2),int(pic_shape[1]*0.2))\n",
    "blockStride = (int(pic_shape[0]*0.2),int(pic_shape[1]*0.2))\n",
    "cellSize = (int(pic_shape[0]*0.1),int(pic_shape[1]*0.1))\n",
    "nbins = 4\n",
    "hog = cv2.HOGDescriptor(winSize, blockSize, blockStride, cellSize, nbins)\n",
    "orb=cv2.ORB_create(nfeatures=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/28500 [00:00<?, ?it/s]C:\\Users\\A.P.Liu Xiaolong\\AppData\\Local\\Temp\\ipykernel_31912\\3950882501.py:28: DeprecationWarning: The normed argument is ignored when density is provided. In future passing both will result in an error.\n",
      "  lbp_hist,_=np.histogram(lbp.reshape((-1,)), normed=True, density=True, bins=256, range=(0, max_bins))\n",
      "100%|██████████| 28500/28500 [04:45<00:00, 99.73it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOG特征维度 (3600,)\n",
      "LBP特征维度 (256,)\n",
      "ORB特征维度 (1600,)\n",
      "GRAY特征维度 (400,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/794 [00:00<?, ?it/s]C:\\Users\\A.P.Liu Xiaolong\\AppData\\Local\\Temp\\ipykernel_31912\\3950882501.py:64: DeprecationWarning: The normed argument is ignored when density is provided. In future passing both will result in an error.\n",
      "  lbp_hist,_ = np.histogram(lbp.reshape((-1,)), normed=True, density=True, bins=256, range=(0, max_bins))\n",
      "100%|██████████| 794/794 [00:08<00:00, 95.66it/s]\n"
     ]
    }
   ],
   "source": [
    "winStride = (8, 8)\n",
    "padding = (8, 8)\n",
    "# 四分割 直方图和词袋模型\n",
    "train_HOG_feature=[]\n",
    "train_ORB_feature=[]\n",
    "train_LBP_feature=[]\n",
    "train_GRAY_feature=[]\n",
    "\n",
    "\n",
    "test_HOG_feature=[]\n",
    "test_ORB_feature=[]\n",
    "test_LBP_feature=[]\n",
    "test_GRAY_feature=[]\n",
    "\n",
    "for img_data in tqdm(train_dataset['data']):\n",
    "    image_segmented = segment_plant(img_data)\n",
    "    image_sharpen = sharpen_image(image_segmented)\n",
    "    gray = cv2.cvtColor(image_sharpen, cv2.COLOR_BGR2GRAY)\n",
    "    # 图像数据生成mask和gray\n",
    "    \n",
    "    #resize后的小型图像\n",
    "    gray_resized=cv2.resize(gray, (20, 20))\n",
    "    train_GRAY_feature.append(gray_resized.reshape((-1,)))\n",
    "    \n",
    "    # lbp统计直方图\n",
    "    lbp = local_binary_pattern(gray,P=8,R=3)\n",
    "    max_bins=lbp.max()\n",
    "    lbp_hist,_=np.histogram(lbp.reshape((-1,)), normed=True, density=True, bins=256, range=(0, max_bins))\n",
    "    train_LBP_feature.append(lbp_hist)\n",
    "    #\n",
    "    # orb特征\n",
    "    ORB_zero=np.zeros((50,32))\n",
    "    kp1, des1 = orb.detectAndCompute(gray, None)\n",
    "    try:\n",
    "        ORB=np.pad(des1,((0,50-des1.shape[0]),(0,0)),'constant')\n",
    "    except:\n",
    "        ORB=np.zeros((50,32))\n",
    "    assert ORB.shape==(50,32)\n",
    "    train_ORB_feature.append(ORB.reshape((-1,)))\n",
    "    # hog特征\n",
    "    #hog_result = hog.compute(image_sharpen, winStride, padding).reshape((-1,))\n",
    "    hog_result = hog.compute(gray, winStride, padding).reshape((-1,))\n",
    "    train_HOG_feature.append(hog_result)\n",
    "\n",
    "print('HOG特征维度',train_HOG_feature[0].shape)\n",
    "print('LBP特征维度',train_LBP_feature[0].shape)\n",
    "print('ORB特征维度',train_ORB_feature[0].shape)\n",
    "print('GRAY特征维度',train_GRAY_feature[0].shape)\n",
    "\n",
    "\n",
    "for img_data in tqdm(test_dataset['data']):\n",
    "    image_segmented = segment_plant(img_data)\n",
    "    image_sharpen = sharpen_image(image_segmented)\n",
    "    gray = cv2.cvtColor(image_sharpen, cv2.COLOR_BGR2GRAY)\n",
    "    # 图像数据生成mask和gray\n",
    "\n",
    "    #resize后的小型图像\n",
    "    gray_resized=cv2.resize(gray, (20, 20))\n",
    "    test_GRAY_feature.append(gray_resized.reshape((-1,)))\n",
    "    \n",
    "    # lbp统计直方图\n",
    "    lbp = local_binary_pattern(gray,P=8,R=3)\n",
    "    max_bins = lbp.max()\n",
    "    lbp_hist,_ = np.histogram(lbp.reshape((-1,)), normed=True, density=True, bins=256, range=(0, max_bins))\n",
    "    test_LBP_feature.append(lbp_hist)\n",
    "    #\n",
    "    # orb特征\n",
    "\n",
    "    kp1, des1 = orb.detectAndCompute(gray, None)\n",
    "    try:\n",
    "        ORB=np.pad(des1,((0,50-des1.shape[0]),(0,0)),'constant')\n",
    "    except:\n",
    "        ORB=np.zeros((50,32))\n",
    "    assert ORB.shape==(50,32)\n",
    "    test_ORB_feature.append(ORB.reshape((-1,)))\n",
    "    # hog特征\n",
    "    #hog_result = hog.compute(image_sharpen, winStride, padding).reshape((-1,))\n",
    "    hog_result = hog.compute(gray, winStride, padding).reshape((-1,))\n",
    "    test_HOG_feature.append(hog_result)\n",
    "# 特征提取\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征归一化完成\n"
     ]
    }
   ],
   "source": [
    "# 特征归一化\n",
    "\n",
    "train_mm_HOG = preprocessing.MinMaxScaler()\n",
    "train_mm_HOG_data = train_mm_HOG.fit_transform(train_HOG_feature)\n",
    "train_HOG_feature = train_mm_HOG.inverse_transform(train_mm_HOG_data)\n",
    "\n",
    "train_mm_LBP = preprocessing.MinMaxScaler()\n",
    "train_mm_LBP_data = train_mm_LBP.fit_transform(train_LBP_feature)\n",
    "train_LBP_feature = train_mm_LBP.inverse_transform(train_mm_LBP_data)\n",
    "\n",
    "train_mm_ORB = preprocessing.MinMaxScaler()\n",
    "train_mm_ORB_data = train_mm_ORB.fit_transform(train_ORB_feature)\n",
    "train_ORB_feature = train_mm_ORB.inverse_transform(train_mm_ORB_data)\n",
    "\n",
    "train_mm_GRAY = preprocessing.MinMaxScaler()\n",
    "train_mm_GRAY_data = train_mm_GRAY.fit_transform(train_GRAY_feature)\n",
    "train_GRAY_feature = train_mm_GRAY.inverse_transform(train_mm_GRAY_data)\n",
    "\n",
    "\n",
    "test_mm_HOG = preprocessing.MinMaxScaler()\n",
    "test_mm_HOG_data = test_mm_HOG.fit_transform(test_HOG_feature)\n",
    "test_HOG_feature = test_mm_HOG.inverse_transform(test_mm_HOG_data)\n",
    "\n",
    "test_mm_LBP = preprocessing.MinMaxScaler()\n",
    "test_mm_LBP_data = test_mm_LBP.fit_transform(test_LBP_feature)\n",
    "test_LBP_feature = test_mm_LBP.inverse_transform(test_mm_LBP_data)\n",
    "\n",
    "test_mm_ORB = preprocessing.MinMaxScaler()\n",
    "test_mm_ORB_data = test_mm_ORB.fit_transform(test_ORB_feature)\n",
    "test_ORB_feature = test_mm_ORB.inverse_transform(test_mm_ORB_data)\n",
    "\n",
    "test_mm_GRAY = preprocessing.MinMaxScaler()\n",
    "test_mm_GRAY_data = test_mm_GRAY.fit_transform(test_GRAY_feature)\n",
    "test_GRAY_feature = test_mm_GRAY.inverse_transform(test_mm_GRAY_data)\n",
    "\n",
    "print('特征归一化完成')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train综合特征维度 (28500, 5456)\n",
      "test综合特征维度 (794, 5456)\n",
      "特征提取结束\n"
     ]
    }
   ],
   "source": [
    "# 特征融合\n",
    "\n",
    "# train_feature=np.hstack([np.array(train_HOG_feature),np.array(train_LBP_feature),np.array(train_ORB_feature),np.array(train_GRAY_feature)])\n",
    "# test_feature=np.hstack([np.array(test_HOG_feature),np.array(test_LBP_feature),np.array(test_ORB_feature),np.array(test_GRAY_feature)])\n",
    "train_feature=np.hstack([np.array(train_HOG_feature),np.array(train_LBP_feature),np.array(train_ORB_feature)])\n",
    "test_feature=np.hstack([np.array(test_HOG_feature),np.array(test_LBP_feature),np.array(test_ORB_feature)])\n",
    "# train_feature=np.hstack([np.array(train_HOG_feature),np.array(train_ORB_feature)])\n",
    "# test_feature=np.hstack([np.array(test_HOG_feature),np.array(test_ORB_feature)])\n",
    "# train_feature=np.array(train_ORB_feature)\n",
    "# test_feature=np.array(test_ORB_feature)\n",
    "# train_feature=np.array(train_HOG_feature)\n",
    "# test_feature=np.array(test_HOG_feature)\n",
    "print('train综合特征维度', train_feature.shape)\n",
    "print('test综合特征维度', test_feature.shape)\n",
    "print('特征提取结束')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据降维开始\n",
      "(29294, 1945)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#数据降维\n",
    "print('数据降维开始')\n",
    "n_components=2000\n",
    "train_len=len(train_feature)\n",
    "data=np.vstack([train_feature,test_feature])\n",
    "# pca_tsne = TSNE(n_components=n_components)\n",
    "# newData = pca_tsne.fit_transform(data)\n",
    "sklearn_kpca = KernelPCA(n_components=n_components, kernel=\"rbf\", gamma=15)\n",
    "newData = sklearn_kpca.fit_transform(data)\n",
    "print(newData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征数据存储结束\n"
     ]
    }
   ],
   "source": [
    "#特征数据存储\n",
    "train_feature_dist=train_dataset.copy()\n",
    "train_feature_dist['data']=train_feature\n",
    "pickle.dump(train_feature_dist,open('E:/py/MachineLearing/MachineLearning-CourseExercise/PlantSeedlingsClassification/train_feature_HOG.pkl','wb'))\n",
    "test_feature_dist=test_dataset.copy()\n",
    "test_feature_dist['data']=test_feature\n",
    "pickle.dump(test_feature_dist,open('E:/py/MachineLearing/MachineLearning-CourseExercise/PlantSeedlingsClassification/test_feature_HOG.pkl','wb'))\n",
    "print('特征数据存储结束')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM分类开始\n",
      "随机森林分类开始\n",
      "Xgboost分类开始\n",
      "测试集分类预测结束\n"
     ]
    }
   ],
   "source": [
    "# SVM分类\n",
    "\n",
    "print('SVM分类开始')\n",
    "\n",
    "modelSVM = svm.SVC()\n",
    "modelSVM.fit(train_feature, train_dataset['target'])\n",
    "predictedSVM = modelSVM.predict(test_feature)\n",
    "\n",
    "# 随机森林分类\n",
    "print('随机森林分类开始')\n",
    "\n",
    "modelRF = RandomForestClassifier()\n",
    "modelRF.fit(train_feature, train_dataset['target'])\n",
    "predictedRF = modelRF.predict(test_feature)\n",
    "\n",
    "\n",
    "#XGBoost分类\n",
    "print('Xgboost分类开始')\n",
    "\n",
    "model = XGBClassifier(max_depth=5)\n",
    "model.fit(train_feature, train_dataset['target'])\n",
    "predictedXG = model.predict(test_feature)\n",
    "\n",
    "\n",
    "print('测试集分类预测结束')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"e:\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 767, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"e:\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 106, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "  File \"e:\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 267, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)\n",
      "  File \"e:\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1136, in f1_score\n",
      "    return fbeta_score(\n",
      "  File \"e:\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1277, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(\n",
      "  File \"e:\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1563, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n",
      "  File \"e:\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1381, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].\n",
      "\n",
      "  warnings.warn(\n",
      "e:\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"e:\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 767, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"e:\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 106, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "  File \"e:\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 267, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)\n",
      "  File \"e:\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1136, in f1_score\n",
      "    return fbeta_score(\n",
      "  File \"e:\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1277, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(\n",
      "  File \"e:\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1563, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n",
      "  File \"e:\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1381, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].\n",
      "\n",
      "  warnings.warn(\n",
      "e:\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"e:\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 767, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"e:\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 106, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "  File \"e:\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 267, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)\n",
      "  File \"e:\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1136, in f1_score\n",
      "    return fbeta_score(\n",
      "  File \"e:\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1277, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(\n",
      "  File \"e:\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1563, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n",
      "  File \"e:\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1381, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].\n",
      "\n",
      "  warnings.warn(\n",
      "e:\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"e:\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 767, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"e:\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 106, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "  File \"e:\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 267, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)\n",
      "  File \"e:\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1136, in f1_score\n",
      "    return fbeta_score(\n",
      "  File \"e:\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1277, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(\n",
      "  File \"e:\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1563, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n",
      "  File \"e:\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1381, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [14], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m     result_list\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msqrt(\u001b[39m-\u001b[39mcross_val_score(my_model, train_feature, train_dataset[\u001b[39m'\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m'\u001b[39m], scoring\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mf1\u001b[39m\u001b[39m\"\u001b[39m, cv \u001b[39m=\u001b[39m kf))\n\u001b[0;32m      4\u001b[0m     \u001b[39mreturn\u001b[39;00m(result_list)\n\u001b[1;32m----> 5\u001b[0m f1_kf(model)\n",
      "Cell \u001b[1;32mIn [14], line 3\u001b[0m, in \u001b[0;36mf1_kf\u001b[1;34m(my_model)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mf1_kf\u001b[39m(my_model):\n\u001b[0;32m      2\u001b[0m     kf \u001b[39m=\u001b[39m KFold(\u001b[39m5\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, random_state\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m)\u001b[39m.\u001b[39mget_n_splits(train_feature)\n\u001b[1;32m----> 3\u001b[0m     result_list\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msqrt(\u001b[39m-\u001b[39mcross_val_score(my_model, train_feature, train_dataset[\u001b[39m'\u001b[39;49m\u001b[39mtarget\u001b[39;49m\u001b[39m'\u001b[39;49m], scoring\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mf1\u001b[39;49m\u001b[39m\"\u001b[39;49m, cv \u001b[39m=\u001b[39;49m kf))\n\u001b[0;32m      4\u001b[0m     \u001b[39mreturn\u001b[39;00m(result_list)\n",
      "File \u001b[1;32me:\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[39m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    513\u001b[0m scorer \u001b[39m=\u001b[39m check_scoring(estimator, scoring\u001b[39m=\u001b[39mscoring)\n\u001b[1;32m--> 515\u001b[0m cv_results \u001b[39m=\u001b[39m cross_validate(\n\u001b[0;32m    516\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m    517\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m    518\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m    519\u001b[0m     groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[0;32m    520\u001b[0m     scoring\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mscore\u001b[39;49m\u001b[39m\"\u001b[39;49m: scorer},\n\u001b[0;32m    521\u001b[0m     cv\u001b[39m=\u001b[39;49mcv,\n\u001b[0;32m    522\u001b[0m     n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    523\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    524\u001b[0m     fit_params\u001b[39m=\u001b[39;49mfit_params,\n\u001b[0;32m    525\u001b[0m     pre_dispatch\u001b[39m=\u001b[39;49mpre_dispatch,\n\u001b[0;32m    526\u001b[0m     error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[0;32m    527\u001b[0m )\n\u001b[0;32m    528\u001b[0m \u001b[39mreturn\u001b[39;00m cv_results[\u001b[39m\"\u001b[39m\u001b[39mtest_score\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32me:\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:266\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    265\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 266\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    268\u001b[0m         clone(estimator),\n\u001b[0;32m    269\u001b[0m         X,\n\u001b[0;32m    270\u001b[0m         y,\n\u001b[0;32m    271\u001b[0m         scorers,\n\u001b[0;32m    272\u001b[0m         train,\n\u001b[0;32m    273\u001b[0m         test,\n\u001b[0;32m    274\u001b[0m         verbose,\n\u001b[0;32m    275\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    276\u001b[0m         fit_params,\n\u001b[0;32m    277\u001b[0m         return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[0;32m    278\u001b[0m         return_times\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    279\u001b[0m         return_estimator\u001b[39m=\u001b[39;49mreturn_estimator,\n\u001b[0;32m    280\u001b[0m         error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[0;32m    281\u001b[0m     )\n\u001b[0;32m    282\u001b[0m     \u001b[39mfor\u001b[39;49;00m train, test \u001b[39min\u001b[39;49;00m cv\u001b[39m.\u001b[39;49msplit(X, y, groups)\n\u001b[0;32m    283\u001b[0m )\n\u001b[0;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    287\u001b[0m \u001b[39m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[39m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[39m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32me:\\Anaconda3\\envs\\torch\\lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32me:\\Anaconda3\\envs\\torch\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32me:\\Anaconda3\\envs\\torch\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32me:\\Anaconda3\\envs\\torch\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32me:\\Anaconda3\\envs\\torch\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32me:\\Anaconda3\\envs\\torch\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32me:\\Anaconda3\\envs\\torch\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32me:\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\utils\\fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    116\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig):\n\u001b[1;32m--> 117\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32me:\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    684\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    685\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 686\u001b[0m         estimator\u001b[39m.\u001b[39;49mfit(X_train, y_train, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[0;32m    688\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    689\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    690\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[1;32me:\\Anaconda3\\envs\\torch\\lib\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[0;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[1;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32me:\\Anaconda3\\envs\\torch\\lib\\site-packages\\xgboost\\sklearn.py:1516\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1488\u001b[0m (\n\u001b[0;32m   1489\u001b[0m     model,\n\u001b[0;32m   1490\u001b[0m     metric,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1495\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[0;32m   1496\u001b[0m )\n\u001b[0;32m   1497\u001b[0m train_dmatrix, evals \u001b[39m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[0;32m   1498\u001b[0m     missing\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmissing,\n\u001b[0;32m   1499\u001b[0m     X\u001b[39m=\u001b[39mX,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1513\u001b[0m     feature_types\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_types,\n\u001b[0;32m   1514\u001b[0m )\n\u001b[1;32m-> 1516\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[0;32m   1517\u001b[0m     params,\n\u001b[0;32m   1518\u001b[0m     train_dmatrix,\n\u001b[0;32m   1519\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_num_boosting_rounds(),\n\u001b[0;32m   1520\u001b[0m     evals\u001b[39m=\u001b[39;49mevals,\n\u001b[0;32m   1521\u001b[0m     early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds,\n\u001b[0;32m   1522\u001b[0m     evals_result\u001b[39m=\u001b[39;49mevals_result,\n\u001b[0;32m   1523\u001b[0m     obj\u001b[39m=\u001b[39;49mobj,\n\u001b[0;32m   1524\u001b[0m     custom_metric\u001b[39m=\u001b[39;49mmetric,\n\u001b[0;32m   1525\u001b[0m     verbose_eval\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m   1526\u001b[0m     xgb_model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m   1527\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m   1528\u001b[0m )\n\u001b[0;32m   1530\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m callable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective):\n\u001b[0;32m   1531\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective \u001b[39m=\u001b[39m params[\u001b[39m\"\u001b[39m\u001b[39mobjective\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32me:\\Anaconda3\\envs\\torch\\lib\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[0;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[1;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32me:\\Anaconda3\\envs\\torch\\lib\\site-packages\\xgboost\\training.py:185\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    184\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m bst\u001b[39m.\u001b[39;49mupdate(dtrain, i, obj)\n\u001b[0;32m    186\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    187\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32me:\\Anaconda3\\envs\\torch\\lib\\site-packages\\xgboost\\core.py:1918\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1915\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_dmatrix_features(dtrain)\n\u001b[0;32m   1917\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1918\u001b[0m     _check_call(_LIB\u001b[39m.\u001b[39;49mXGBoosterUpdateOneIter(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[0;32m   1919\u001b[0m                                             ctypes\u001b[39m.\u001b[39;49mc_int(iteration),\n\u001b[0;32m   1920\u001b[0m                                             dtrain\u001b[39m.\u001b[39;49mhandle))\n\u001b[0;32m   1921\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1922\u001b[0m     pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(dtrain, output_margin\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def f1_kf(my_model):\n",
    "    kf = KFold(5, shuffle=True, random_state=50).get_n_splits(train_feature)\n",
    "    result_list= np.sqrt(-cross_val_score(my_model, train_feature, train_dataset['target'], scoring=\"f1\", cv = kf))\n",
    "    return(result_list)\n",
    "f1_kf(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 结果生成\n",
    "# pred = np.exp(predicted)\n",
    "# print(predicted)\n",
    "# subSVM=pd.read_csv('E:/py/MachineLearing/MachineLearning-CourseExercise/PlantSeedlingsClassification/sample_submission.csv')\n",
    "# subSVM['file'] = test_dataset['file_name']\n",
    "# subSVM['species'] = list(map(lambda x:train_dataset['dict'][x], predictedSVM))\n",
    "# subSVM.to_csv('E:/py/MachineLearing/MachineLearning-CourseExercise/PlantSeedlingsClassification/submission_SVM.csv', index=False)\n",
    "\n",
    "# subRF=pd.read_csv('E:/py/MachineLearing/MachineLearning-CourseExercise/PlantSeedlingsClassification/sample_submission.csv')\n",
    "# subRF['file'] = test_dataset['file_name']\n",
    "# subRF['species'] = list(map(lambda x:train_dataset['dict'][x], predictedRF))\n",
    "# subRF.to_csv('E:/py/MachineLearing/MachineLearning-CourseExercise/PlantSeedlingsClassification/submission_RF.csv', index=False)\n",
    "\n",
    "subXG=pd.read_csv('E:/py/MachineLearing/MachineLearning-CourseExercise/PlantSeedlingsClassification/sample_submission.csv')\n",
    "subXG['file'] = test_dataset['file_name']\n",
    "subXG['species'] = list(map(lambda x:train_dataset['dict'][x], predictedXG))\n",
    "subXG.to_csv('E:/py/MachineLearing/MachineLearning-CourseExercise/PlantSeedlingsClassification/submission_XG.csv', index=False)\n",
    "\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "49844d814d8b3527cdd6e0ee9ead42f5b31a2a1e9678336de9e62eb2a92861cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
