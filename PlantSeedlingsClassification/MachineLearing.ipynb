{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from skimage.feature import local_binary_pattern\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plantimg_shape = (200, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集长度: 4750 测试集长度: 794\n"
     ]
    }
   ],
   "source": [
    "train_plantimage = pickle.load(open('E:/py/MachineLearing/MachineLearning-CourseExercise/my_train.pkl', 'rb'))\n",
    "test_plantimage = pickle.load(open('E:/py/MachineLearing/MachineLearning-CourseExercise/my_test.pkl', 'rb'))\n",
    "print('训练集长度:', len(train_plantimage['data']), '测试集长度:', len(test_plantimage['data']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据拓展\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(train_plantimage['data'])):\n",
    "    img=train_plantimage['data'][i]\n",
    "    target=train_plantimage['target'][i]\n",
    "    \n",
    "    # 翻转\n",
    "    train_plantimage['data'].append(cv2.flip(img, -1))\n",
    "    train_plantimage['target'].append(target)\n",
    "    train_plantimage['data'].append(cv2.flip(img, 1))\n",
    "    train_plantimage['target'].append(target)\n",
    "\n",
    "    # 旋转\n",
    "    # getRotationMatrix2D(旋转中心,旋转角度,缩放比例)\n",
    "    RotationMatrix = cv2.getRotationMatrix2D((int(plantimg_shape[0]*0.5),int(plantimg_shape[1]*0.5)), 45, 1)\n",
    "    warpAffine = cv2.warpAffine(img, RotationMatrix, plantimg_shape)\n",
    "    train_plantimage['data'].append(warpAffine)\n",
    "    train_plantimage['target'].append(target)\n",
    "    RotationMatrix = cv2.getRotationMatrix2D((int(plantimg_shape[0]*0.5),int(plantimg_shape[1]*0.5)), 90, 1)\n",
    "    warpAffine = cv2.warpAffine(img, RotationMatrix, plantimg_shape)\n",
    "    train_plantimage['data'].append(warpAffine)\n",
    "    train_plantimage['target'].append(target)\n",
    "    RotationMatrix = cv2.getRotationMatrix2D((int(plantimg_shape[0]*0.5),int(plantimg_shape[1]*0.5)), 135, 1)\n",
    "    warpAffine = cv2.warpAffine(img, RotationMatrix, plantimg_shape)\n",
    "    train_plantimage['data'].append(warpAffine)\n",
    "    train_plantimage['target'].append(target)\n",
    "\n",
    "#\n",
    "print('数据拓展')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plantimg_mask(image):\n",
    "    image_hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    # 突出绿色部分\n",
    "    sensitivity = 35\n",
    "    lower_hsv = np.array([60 - sensitivity, 100, 50])\n",
    "    upper_hsv = np.array([60 + sensitivity, 255, 255])\n",
    "    # 输出的图片为二值化图只有黑白两种颜色\n",
    "    mask = cv2.inRange(image_hsv, lower_hsv, upper_hsv)\n",
    "\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11, 11))\n",
    "    # 形态学滤波\n",
    "    # cv2.morphologyEx(img, op, kernel)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def plantimg_GreenPlant(image):\n",
    "    mask = plantimg_mask(image)\n",
    "    output = cv2.bitwise_and(image, image, mask=mask)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plantimg_GuassProcess(image):\n",
    "    # cv2.GaussianBlur(SRC,ksize,sigmaX [,DST [,sigmaY [,borderType ] ] ] ) \n",
    "    # 减少噪声\n",
    "    image_blurred = cv2.GaussianBlur(image, (0, 0), 3)\n",
    "    image_Guass = cv2.addWeighted(image, 1.5, image_blurred, -0.5, 0)\n",
    "    return image_Guass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv2.MORPH_OPEN\t开运算(open) ,先腐蚀后膨胀的过程。开运算可以用来消除小黑点，在纤细点处分离物体、平滑较大物体的边界的 同时并不明显改变其面积。\n",
    "# cv2.MORPH_CLOSE\t闭运算(close)，先膨胀后腐蚀的过程。闭运算可以用来排除小黑洞。\n",
    "# cv2.MORPH_GRADIENT\t形态学梯度(morph-grad)，可以突出团块(blob)的边缘，保留物体的边缘轮廓。\n",
    "# cv2.MORPH_TOPHAT\t顶帽(top-hat)，将突出比原轮廓亮的部分。\n",
    "# cv2.MORPH_BLACKHAT\t黑帽(black-hat)，将突出比原轮廓暗的部分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "winSize = plantimg_shape\n",
    "blockSize = (int(plantimg_shape[0]*0.2),int(plantimg_shape[1]*0.2))\n",
    "blockStride = (int(plantimg_shape[0]*0.2),int(plantimg_shape[1]*0.2))\n",
    "cellSize = (int(plantimg_shape[0]*0.1),int(plantimg_shape[1]*0.1))\n",
    "nbins = 4\n",
    "\n",
    "# cv2.HOGDescriptor(winSize,blockSize,blockStride,cellSize,nbins,derivAperture,winSigma,histogramNormType,L2HysThreshold,gammaCorrection,nlevels)\n",
    "hog = cv2.HOGDescriptor(winSize, blockSize, blockStride, cellSize, nbins)\n",
    "\n",
    "# cv2.ORB_create(nfeatures = 500,scaleFactor = 1.2,nlevels = 8,edgeThreshold = 31,firstLevel = 0,WTA_K = 2,scoreType = HARRIS_SCORE,patchSize = 31,fastThreshold = 20)\n",
    "orb=cv2.ORB_create(nfeatures=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nfeatures ：最多提取的特征点的数量；\n",
    "# scaleFactor ： 金字塔图像之间的尺度参数，类似于SIFT中的k；\n",
    "# nlevels： 高斯金字塔的层数；\n",
    "# edgeThreshold ：边缘阈值，这个值主要是根据后面的patchSize来定的，靠近边缘edgeThreshold以内的像素是不检测特征点的。\n",
    "# firstLevel-：看过SIFT都知道，我们可以指定第一层的索引值，这里默认为0。\n",
    "# WET_K ： 用于产生BIREF描述子的点对的个数，一般为2个，也可以设置为3个或4个，那么这时候描述子之间的距离计算就不能用汉明距离了，而是应该用一个变种。OpenCV中，如果设置WET_K = 2，则选用点对就只有2个点，匹配的时候距离参数选择NORM_HAMMING，如果WET_K设置为3或4，则BIREF描述子会选择3个或4个点，那么后面匹配的时候应该选择的距离参数为NORM_HAMMING2。\n",
    "# scoreType ：用于对特征点进行排序的算法，你可以选择HARRIS_SCORE，也可以选择FAST_SCORE，但是它也只是比前者快一点点而已。\n",
    "# patchSize ：用于计算BIREF描述子的特征点邻域大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集特征提取\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28500/28500 [05:04<00:00, 93.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集HOG维度:(3600,)\n",
      "训练集LBP维度:(256,)\n",
      "训练集ORB维度:(1600,)\n",
      "训练集GRAY维度:(400,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 特征提取\n",
    "winStride = (8, 8)\n",
    "padding = (8, 8)\n",
    "\n",
    "plantimg_train_HOG=[]\n",
    "plantimg_train_ORB=[]\n",
    "plantimg_train_LBP=[]\n",
    "plantimg_train_Origin_GRAY=[]\n",
    "\n",
    "print(\"训练集特征提取\")\n",
    "\n",
    "for img_data in tqdm(train_plantimage['data']):\n",
    "    image_GreenPlant = plantimg_GreenPlant(img_data)\n",
    "    image_Guass = plantimg_GuassProcess(image_GreenPlant)\n",
    "    gray = cv2.cvtColor(image_Guass, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # 生成gray\n",
    "    gray_resized=cv2.resize(gray, (20, 20))\n",
    "    plantimg_train_Origin_GRAY.append(gray_resized.reshape((-1,)))\n",
    "    \n",
    "    # LBP\n",
    "    lbp = local_binary_pattern(gray,P=8,R=3)\n",
    "    max_bins=lbp.max()\n",
    "    lbp_hist,_=np.histogram(lbp.reshape((-1,)), normed=True, density=True, bins=256, range=(0, max_bins))\n",
    "    plantimg_train_LBP.append(lbp_hist)\n",
    "    \n",
    "    # ORB\n",
    "    ORB_zero=np.zeros((50,32))\n",
    "    kpsA, descsA = orb.detectAndCompute(gray, None)\n",
    "    try:\n",
    "        ORB=np.pad(descsA,((0,50-descsA.shape[0]),(0,0)),'constant')\n",
    "    except:\n",
    "        ORB=np.zeros((50,32))\n",
    "    assert ORB.shape==(50,32)\n",
    "    plantimg_train_ORB.append(ORB.reshape((-1,)))\n",
    "    \n",
    "    # HOG\n",
    "    \n",
    "    #hog_result = hog.compute(image_Guass, winStride, padding).reshape((-1,))\n",
    "    hog_result = hog.compute(gray, winStride, padding).reshape((-1,))\n",
    "    plantimg_train_HOG.append(hog_result)\n",
    "\n",
    "print('训练集HOG维度:{}'.format(plantimg_train_HOG[0].shape))\n",
    "print('训练集LBP维度:{}'.format(plantimg_train_LBP[0].shape))\n",
    "print('训练集ORB维度:{}'.format(plantimg_train_ORB[0].shape))\n",
    "print('训练集GRAY维度:{}'.format(plantimg_train_Origin_GRAY[0].shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集特征提取\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 794/794 [00:09<00:00, 80.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集HOG维度:(3600,)\n",
      "测试集LBP维度:(256,)\n",
      "测试集ORB维度:(1600,)\n",
      "测试集GRAY维度:(400,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "plantimg_test_HOG=[]\n",
    "plantimg_test_ORB=[]\n",
    "plantimg_test_LBP=[]\n",
    "plantimg_test_Origin_GRAY=[]\n",
    "\n",
    "print(\"测试集特征提取\")\n",
    "\n",
    "for img_data in tqdm(test_plantimage['data']):\n",
    "    image_GreenPlant = plantimg_GreenPlant(img_data)\n",
    "    image_Guass = plantimg_GuassProcess(image_GreenPlant)\n",
    "    gray = cv2.cvtColor(image_Guass, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # 生成gray\n",
    "    gray_resized=cv2.resize(gray, (20, 20))\n",
    "    plantimg_test_Origin_GRAY.append(gray_resized.reshape((-1,)))\n",
    "    \n",
    "    # LBP\n",
    "    lbp = local_binary_pattern(gray,P=8,R=3)\n",
    "    max_bins = lbp.max()\n",
    "    lbp_hist,_ = np.histogram(lbp.reshape((-1,)), normed=True, density=True, bins=256, range=(0, max_bins))\n",
    "    plantimg_test_LBP.append(lbp_hist)\n",
    "    \n",
    "    # ORB\n",
    "\n",
    "    kpsA, descsA = orb.detectAndCompute(gray, None)\n",
    "    try:\n",
    "        ORB=np.pad(descsA,((0,50-descsA.shape[0]),(0,0)),'constant')\n",
    "    except:\n",
    "        ORB=np.zeros((50,32))\n",
    "    assert ORB.shape==(50,32)\n",
    "    plantimg_test_ORB.append(ORB.reshape((-1,)))\n",
    "    \n",
    "    # HOG\n",
    "    #hog_result = hog.compute(image_Guass, winStride, padding).reshape((-1,))\n",
    "    hog_result = hog.compute(gray, winStride, padding).reshape((-1,))\n",
    "    plantimg_test_HOG.append(hog_result)\n",
    "\n",
    "print('测试集HOG维度:{}'.format(plantimg_test_HOG[0].shape))\n",
    "print('测试集LBP维度:{}'.format(plantimg_test_LBP[0].shape))\n",
    "print('测试集ORB维度:{}'.format(plantimg_test_ORB[0].shape))\n",
    "print('测试集GRAY维度:{}'.format(plantimg_test_Origin_GRAY[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集特征归一化开始\n",
      "训练集特征归一化结束\n"
     ]
    }
   ],
   "source": [
    "# 特征归一化\n",
    "# 使用MinMaxScaler()\n",
    "# sklearn.preprocessing.MinMaxScaler(feature_range=(0, 1), copy=True)\n",
    "# 最小-最大规范化对原始数据进行线性变换，变换到[0,1]区间（也可以是其他固定最小最大值的区间）每个特征中的最小值变成了0，最大值变成了1.\n",
    "print('训练集特征归一化开始')\n",
    "plantimg_train__MinMax_HOG = preprocessing.MinMaxScaler()\n",
    "plantimg_train__MinMax_HOG_data = plantimg_train__MinMax_HOG.fit_transform(plantimg_train_HOG)\n",
    "plantimg_train_HOG = plantimg_train__MinMax_HOG.inverse_transform(plantimg_train__MinMax_HOG_data)\n",
    "\n",
    "plantimg_train__MinMax_LBP = preprocessing.MinMaxScaler()\n",
    "plantimg_train__MinMax_LBP_data = plantimg_train__MinMax_LBP.fit_transform(plantimg_train_LBP)\n",
    "plantimg_train_LBP = plantimg_train__MinMax_LBP.inverse_transform(plantimg_train__MinMax_LBP_data)\n",
    "\n",
    "plantimg_train__MinMax_ORB = preprocessing.MinMaxScaler()\n",
    "plantimg_train__MinMax_ORB_data = plantimg_train__MinMax_ORB.fit_transform(plantimg_train_ORB)\n",
    "plantimg_train_ORB = plantimg_train__MinMax_ORB.inverse_transform(plantimg_train__MinMax_ORB_data)\n",
    "\n",
    "plantimg_train__MinMax_Origin_GRAY = preprocessing.MinMaxScaler()\n",
    "plantimg_train__MinMax_Origin_GRAY_data = plantimg_train__MinMax_Origin_GRAY.fit_transform(plantimg_train_Origin_GRAY)\n",
    "plantimg_train_Origin_GRAY = plantimg_train__MinMax_Origin_GRAY.inverse_transform(plantimg_train__MinMax_Origin_GRAY_data)\n",
    "\n",
    "\n",
    "print('训练集特征归一化结束')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集特征归一化开始\n",
      "测试集特征归一化结束\n"
     ]
    }
   ],
   "source": [
    "print('测试集特征归一化开始')\n",
    "plantimg_test__MinMax_HOG = preprocessing.MinMaxScaler()\n",
    "plantimg_test__MinMax_HOG_data = plantimg_test__MinMax_HOG.fit_transform(plantimg_test_HOG)\n",
    "plantimg_test_HOG = plantimg_test__MinMax_HOG.inverse_transform(plantimg_test__MinMax_HOG_data)\n",
    "\n",
    "plantimg_test__MinMax_LBP = preprocessing.MinMaxScaler()\n",
    "plantimg_test__MinMax_LBP_data = plantimg_test__MinMax_LBP.fit_transform(plantimg_test_LBP)\n",
    "plantimg_test_LBP = plantimg_test__MinMax_LBP.inverse_transform(plantimg_test__MinMax_LBP_data)\n",
    "\n",
    "plantimg_test__MinMax_ORB = preprocessing.MinMaxScaler()\n",
    "plantimg_test__MinMax_ORB_data = plantimg_test__MinMax_ORB.fit_transform(plantimg_test_ORB)\n",
    "plantimg_test_ORB = plantimg_test__MinMax_ORB.inverse_transform(plantimg_test__MinMax_ORB_data)\n",
    "\n",
    "plantimg_test__MinMax_Origin_GRAY = preprocessing.MinMaxScaler()\n",
    "plantimg_test__MinMax_Origin_GRAY_data = plantimg_test__MinMax_Origin_GRAY.fit_transform(plantimg_test_Origin_GRAY)\n",
    "plantimg_test_Origin_GRAY = plantimg_test__MinMax_Origin_GRAY.inverse_transform(plantimg_test__MinMax_Origin_GRAY_data)\n",
    "\n",
    "print('测试集特征归一化结束')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征融合开始\n",
      "train HOG特征维度 (28500, 3600)\n",
      "test HOG特征维度 (794, 3600)\n",
      "train 融合3特征维度 (28500, 5456)\n",
      "test 融合3特征维度 (794, 5456)\n",
      "train融合特征维度 (28500, 5856)\n",
      "test融合特征维度 (794, 5856)\n",
      "特征提取结束\n"
     ]
    }
   ],
   "source": [
    "# 特征融合\n",
    "print('特征融合开始')\n",
    "\n",
    "plantimg_train_feature=np.hstack([np.array(plantimg_train_HOG),np.array(plantimg_train_LBP),np.array(plantimg_train_ORB),np.array(plantimg_train_Origin_GRAY)])\n",
    "plantimg_test_feature=np.hstack([np.array(plantimg_test_HOG),np.array(plantimg_test_LBP),np.array(plantimg_test_ORB),np.array(plantimg_test_Origin_GRAY)])\n",
    "\n",
    "plantimg_train_feature_3=np.hstack([np.array(plantimg_train_HOG),np.array(plantimg_train_LBP),np.array(plantimg_train_ORB)])\n",
    "plantimg_test_feature_3=np.hstack([np.array(plantimg_test_HOG),np.array(plantimg_test_LBP),np.array(plantimg_test_ORB)])\n",
    "\n",
    "plantimg_train_feature_HOG=np.array(plantimg_train_HOG)\n",
    "plantimg_test_feature_HOG=np.array(plantimg_test_HOG)\n",
    "\n",
    "print('train HOG特征维度', plantimg_train_feature_HOG.shape)\n",
    "print('test HOG特征维度', plantimg_test_feature_HOG.shape)\n",
    "\n",
    "print('train 融合3特征维度', plantimg_train_feature_3.shape)\n",
    "print('test 融合3特征维度', plantimg_test_feature_3.shape)\n",
    "\n",
    "print('train融合特征维度', plantimg_train_feature.shape)\n",
    "print('test融合特征维度', plantimg_test_feature.shape)\n",
    "\n",
    "print('特征提取结束')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据降维开始\n",
      "(29294, 3)\n"
     ]
    }
   ],
   "source": [
    "# #数据降维\n",
    "# print('数据降维开始')\n",
    "# n_components=2000\n",
    "# train_len=len(plantimg_train_feature)\n",
    "# data=np.vstack([plantimg_train_feature,plantimg_test_feature])\n",
    "\n",
    "# pca_tsne = TSNE(n_components=3)\n",
    "# LOWERData_linear = pca_tsne.fit_transform(data)\n",
    "\n",
    "# # sklearn_kpca = KernelPCA(n_components=n_components, kernel=\"rbf\", gamma=15)\n",
    "# # LOWERData_nonlinear = sklearn_kpca.fit_transform(data)\n",
    "\n",
    "# LOWERData = LOWERData_linear\n",
    "# print(LOWERData.shape)\n",
    "# print('数据降维结束')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主成分分析（PCA）是使用线性映射将数据进行降维，但是通常情况下高维到低维是非线性的，往往达不到预期的结果。核主成分分析（KPCA）将原始数据通过选择适当的核函数（Kernel）映射到高维空间，再利用高维度空间进行线性降维，是一种用于非线性分类的降维工具。因此 KPCA的核心就是核函数。同时，KPCA采用了比较复杂的非线性映射，提高了非线性数据的处理效率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 降维后的数据\n",
    "# plantimg_train_feature_low=LOWERData[0:train_len]\n",
    "# assert train_len==len(plantimg_train_feature_low)\n",
    "# plantimg_test_feature_low=LOWERData[train_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# #特征数据存储\n",
    "# print('特征数据存储开始')\n",
    "# plantimg_train_feature_dist=train_plantimage.copy()\n",
    "# plantimg_train_feature_dist['data']=plantimg_train_feature\n",
    "# pickle.dump(plantimg_train_feature_dist,open('E:/py/MachineLearing/MachineLearning-CourseExercise/PlantSeedlingsClassification/plantimg_train_feature_HOG.pkl','wb'))\n",
    "# plantimg_test_feature_dist=test_plantimage.copy()\n",
    "# plantimg_test_feature_dist['data']=plantimg_test_feature\n",
    "# pickle.dump(plantimg_test_feature_dist,open('E:/py/MachineLearing/MachineLearning-CourseExercise/PlantSeedlingsClassification/plantimg_test_feature_HOG.pkl','wb'))\n",
    "# print('特征数据存储结束')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM分类开始\n",
      "114514\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # SVM分类(降维)\n",
    "\n",
    "# print('SVM分类开始')\n",
    "\n",
    "# modelSVM = svm.SVC()\n",
    "# modelSVM.fit(plantimg_train_feature_low, train_plantimage['target'])\n",
    "# predictedSVM = modelSVM.predict(plantimg_test_feature_low)\n",
    "\n",
    "# print('114514\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM分类开始\n",
      "114514\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVM分类\n",
    "\n",
    "print('SVM分类开始')\n",
    "# ‘linear’:线性核函数 ‘poly’：多项式核函数 ‘rbf’：径像核函数/高斯核 ‘sigmod’:sigmod核函数 ‘precomputed’:核矩阵\n",
    "\n",
    "modelSVM = svm.SVC(kernel='poly', verbose = False,coef0 = 0.0 , C = 10.0 ,degree = 4)\n",
    "modelSVM.fit(plantimg_train_feature, train_plantimage['target'])\n",
    "predictedSVM = modelSVM.predict(plantimg_test_feature)\n",
    "\n",
    "# modelSVM_rbf = svm.SVC(kernel='rbf',verbose = False,coef0 = 0.0)\n",
    "# modelSVM_rbf.fit(plantimg_train_feature, train_plantimage['target'])\n",
    "# predictedSVM_rbf = modelSVM_rbf.predict(plantimg_test_feature)\n",
    "\n",
    "\n",
    "print('114514\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF分类开始\n",
      "114514\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # 随机森林分类(降维)\n",
    "# print('RF分类开始')\n",
    "\n",
    "# modelRF = RandomForestClassifier()\n",
    "# modelRF.fit(plantimg_train_feature_low, train_plantimage['target'])\n",
    "# predictedRF = modelRF.predict(plantimg_test_feature_low)\n",
    "\n",
    "# print('114514\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF分类开始\n",
      "114514\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 随机森林分类\n",
    "print('RF分类开始')\n",
    "\n",
    "modelRF = RandomForestClassifier()\n",
    "modelRF.fit(plantimg_train_feature, train_plantimage['target'])\n",
    "predictedRF = modelRF.predict(plantimg_test_feature)\n",
    "\n",
    "print('114514\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgboost分类开始\n",
      "114514\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# #XGBoost分类(降维)\n",
    "# print('Xgboost分类开始')\n",
    "\n",
    "# model = XGBClassifier(max_depth=5)\n",
    "# model.fit(plantimg_train_feature_low, train_plantimage['target'])\n",
    "# predictedXG = model.predict(plantimg_test_feature_low)\n",
    "\n",
    "# print('114514\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgboost分类开始\n",
      "114514\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#XGBoost分类\n",
    "print('Xgboost分类开始')\n",
    "\n",
    "modelXG = XGBClassifier(max_depth=5)\n",
    "modelXG.fit(plantimg_train_feature, train_plantimage['target'])\n",
    "predictedXG = modelXG.predict(plantimg_test_feature)\n",
    "\n",
    "print('114514\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def model_kf(my_model):\n",
    "#     kf = KFold(5, shuffle=True, random_state=50).get_n_splits(plantimg_train_feature)\n",
    "#     result_list= np.sqrt(-cross_val_score(my_model, plantimg_train_feature, train_plantimage['target'], scoring=\"f1\", cv = kf))\n",
    "#     return(result_list)\n",
    "# model_kf(model)\n",
    "\n",
    "# cross_val_score(estimator, X, y=None, *, groups=None, scoring=None, cv=None, n_jobs=None, verbose=0, fit_params=None, pre_dispatch=\"2*n_jobs\", error_score=np.nan)\n",
    "# estimator：估计器，也就是模型\n",
    "# X, y：数据，标签值\n",
    "# soring：调用的方法\n",
    "# cv：交叉验证生成器或可迭代的次数\n",
    "# n_jobs：同时工作的cpu个数（-1代表全部）\n",
    "# verbose：日志冗长度，int：冗长度，0：不输出训练过程，1：偶尔输出，>1：对每个子模型都输出\n",
    "# fit_params：传递给估计器的拟合方法的参数\n",
    "# pre_dispatch：控制并行执行期间调度的作业数量。减少这个数量对于避免在CPU发送更多作业时CPU内存消耗的扩大是有用的。\n",
    "\n",
    "# scoring参数 accuracy average_percision f1 f1_micro f1_macro f1_weighted f1_samples neg_log_loss precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "结果写入到csv文件\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# pred = np.exp(predicted)\n",
    "# print(predicted)\n",
    "\n",
    "#结果生成\n",
    "subSVM=pd.read_csv('E:/py/MachineLearing/MachineLearning-CourseExercise/PlantSeedlingsClassification/sample_submission.csv')\n",
    "subSVM['file'] = test_plantimage['file_name']\n",
    "subSVM['species'] = list(map(lambda x:train_plantimage['dict'][x], predictedSVM))\n",
    "subSVM.to_csv('E:/py/MachineLearing/MachineLearning-CourseExercise/PlantSeedlingsClassification/submission_SVM.csv', index=False)\n",
    "print(\"结果写入到csv文件\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "结果写入到csv文件\n"
     ]
    }
   ],
   "source": [
    "# 结果生成\n",
    "subRF=pd.read_csv('E:/py/MachineLearing/MachineLearning-CourseExercise/PlantSeedlingsClassification/sample_submission.csv')\n",
    "subRF['file'] = test_plantimage['file_name']\n",
    "subRF['species'] = list(map(lambda x:train_plantimage['dict'][x], predictedRF))\n",
    "subRF.to_csv('E:/py/MachineLearing/MachineLearning-CourseExercise/PlantSeedlingsClassification/submission_RF.csv', index=False)\n",
    "print(\"结果写入到csv文件\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "结果写入到csv文件\n"
     ]
    }
   ],
   "source": [
    "# 结果生成\n",
    "subXG=pd.read_csv('E:/py/MachineLearing/MachineLearning-CourseExercise/PlantSeedlingsClassification/sample_submission.csv')\n",
    "subXG['file'] = test_plantimage['file_name']\n",
    "subXG['species'] = list(map(lambda x:train_plantimage['dict'][x], predictedXG))\n",
    "subXG.to_csv('E:/py/MachineLearing/MachineLearning-CourseExercise/PlantSeedlingsClassification/submission_XG.csv', index=False)\n",
    "print(\"结果写入到csv文件\")\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "pickle.dump(modelSVM,open(\"E:/py/MachineLearing/MachineLearning-CourseExercise/PlantSeedlingsClassification/modelSVM.pth\",\"wb\"))\n",
    "pickle.dump(modelRF,open(\"E:/py/MachineLearing/MachineLearning-CourseExercise/PlantSeedlingsClassification/modelRF.pth\",\"wb\"))\n",
    "pickle.dump(modelXG,open(\"E:/py/MachineLearing/MachineLearning-CourseExercise/PlantSeedlingsClassification/modelXG.pth\",\"wb\")) \n",
    "modelXG.save_model('E:/py/MachineLearing/MachineLearning-CourseExercise/PlantSeedlingsClassification/XGBoost.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取模型\n",
    "\n",
    "# loaded_modelXG = pickle.load(open(\"E:/py/MachineLearing/MachineLearning-CourseExercise/PlantSeedlingsClassification/modelXG.pth\",\"rb\"))\n",
    "# loaded_modelRF = pickle.load(open(\"E:/py/MachineLearing/MachineLearning-CourseExercise/PlantSeedlingsClassification/modelRF.pth\",\"rb\"))\n",
    "# loaded_modelSVM = pickle.load(open(\"E:/py/MachineLearing/MachineLearning-CourseExercise/PlantSeedlingsClassification/modelSVM.pth\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM分类开始\n",
      "结果写入到csv文件\n",
      "结果写入到csv文件\n"
     ]
    }
   ],
   "source": [
    "# 模型对比\n",
    "print('SVM分类开始')\n",
    "\n",
    "modelSVM_poly_3 = svm.SVC(kernel='poly', verbose = False,coef0 = 0.0 , C = 10.0 ,degree = 3)\n",
    "modelSVM_poly_3.fit(plantimg_train_feature_3, train_plantimage['target'])\n",
    "predictedSVM_poly_3 = modelSVM_poly_3.predict(plantimg_test_feature_3)\n",
    "\n",
    "subSVM_poly_3=pd.read_csv('E:/py/MachineLearing/MachineLearning-CourseExercise/PlantSeedlingsClassification/sample_submission.csv')\n",
    "subSVM_poly_3['file'] = test_plantimage['file_name']\n",
    "subSVM_poly_3['species'] = list(map(lambda x:train_plantimage['dict'][x], predictedSVM_poly_3))\n",
    "subSVM_poly_3.to_csv('E:/py/MachineLearing/MachineLearning-CourseExercise/PlantSeedlingsClassification/submission_SVM_poly_3.csv', index=False)\n",
    "\n",
    "print(\"结果写入到csv文件\")\n",
    "\n",
    "modelSVM_poly_HOG = svm.SVC(kernel='poly', verbose = False,coef0 = 0.0 , C = 10.0 ,degree = 3)\n",
    "modelSVM_poly_HOG.fit(plantimg_train_feature_HOG, train_plantimage['target'])\n",
    "predictedSVM_poly_HOG = modelSVM_poly_HOG.predict(plantimg_test_feature_HOG)\n",
    "\n",
    "subSVM_poly_HOG=pd.read_csv('E:/py/MachineLearing/MachineLearning-CourseExercise/PlantSeedlingsClassification/sample_submission.csv')\n",
    "subSVM_poly_HOG['file'] = test_plantimage['file_name']\n",
    "subSVM_poly_HOG['species'] = list(map(lambda x:train_plantimage['dict'][x], predictedSVM_poly_HOG))\n",
    "subSVM_poly_HOG.to_csv('E:/py/MachineLearing/MachineLearning-CourseExercise/PlantSeedlingsClassification/submission_SVM_poly_HOG.csv', index=False)\n",
    "\n",
    "print(\"结果写入到csv文件\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型对比\n",
    "print('XGBoost分类开始')\n",
    "\n",
    "modelXG_3 = XGBClassifier(max_depth=5)\n",
    "modelXG_3.fit(plantimg_train_feature_3, train_plantimage['target'])\n",
    "predictedXG_3 = modelXG_3.predict(plantimg_test_feature_3)\n",
    "\n",
    "subXG_3=pd.read_csv('E:/py/MachineLearing/MachineLearning-CourseExercise/PlantSeedlingsClassification/sample_submission.csv')\n",
    "subXG_3['file'] = test_plantimage['file_name']\n",
    "subXG_3['species'] = list(map(lambda x:train_plantimage['dict'][x], predictedXG_3))\n",
    "subXG_3.to_csv('E:/py/MachineLearing/MachineLearning-CourseExercise/PlantSeedlingsClassification/submission_XG_3.csv', index=False)\n",
    "print(\"结果写入到csv文件\")\n",
    "\n",
    "modelXG_HOG = XGBClassifier(max_depth=5)\n",
    "modelXG_HOG.fit(plantimg_train_feature_HOG, train_plantimage['target'])\n",
    "predictedXG_HOG = modelXG_HOG.predict(plantimg_test_feature_HOG)\n",
    "\n",
    "subXG_HOG=pd.read_csv('E:/py/MachineLearing/MachineLearning-CourseExercise/PlantSeedlingsClassification/sample_submission.csv')\n",
    "subXG_HOG['file'] = test_plantimage['file_name']\n",
    "subXG_HOG['species'] = list(map(lambda x:train_plantimage['dict'][x], predictedXG_HOG))\n",
    "subXG_HOG.to_csv('E:/py/MachineLearing/MachineLearning-CourseExercise/PlantSeedlingsClassification/submission_XG_HOG.csv', index=False)\n",
    "print(\"结果写入到csv文件\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF分类开始\n",
      "结果写入到csv文件\n",
      "结果写入到csv文件\n"
     ]
    }
   ],
   "source": [
    "# 模型对比\n",
    "print('RF分类开始')\n",
    "modelRF_3 = RandomForestClassifier()\n",
    "modelRF_3.fit(plantimg_train_feature_3, train_plantimage['target'])\n",
    "predictedRF_3 = modelRF_3.predict(plantimg_test_feature_3)\n",
    "\n",
    "subRF_3=pd.read_csv('E:/py/MachineLearing/MachineLearning-CourseExercise/PlantSeedlingsClassification/sample_submission.csv')\n",
    "subRF_3['file'] = test_plantimage['file_name']\n",
    "subRF_3['species'] = list(map(lambda x:train_plantimage['dict'][x], predictedRF_3))\n",
    "subRF_3.to_csv('E:/py/MachineLearing/MachineLearning-CourseExercise/PlantSeedlingsClassification/submission_RF_3.csv', index=False)\n",
    "print(\"结果写入到csv文件\")\n",
    "\n",
    "modelRF_HOG = RandomForestClassifier()\n",
    "modelRF_HOG.fit(plantimg_train_feature_HOG, train_plantimage['target'])\n",
    "predictedRF_HOG = modelRF_HOG.predict(plantimg_test_feature_HOG)\n",
    "\n",
    "subRF_HOG=pd.read_csv('E:/py/MachineLearing/MachineLearning-CourseExercise/PlantSeedlingsClassification/sample_submission.csv')\n",
    "subRF_HOG['file'] = test_plantimage['file_name']\n",
    "subRF_HOG['species'] = list(map(lambda x:train_plantimage['dict'][x], predictedRF_HOG))\n",
    "subRF_HOG.to_csv('E:/py/MachineLearing/MachineLearning-CourseExercise/PlantSeedlingsClassification/submission_RF_HOG.csv', index=False)\n",
    "print(\"结果写入到csv文件\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 05:59:00) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "49844d814d8b3527cdd6e0ee9ead42f5b31a2a1e9678336de9e62eb2a92861cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
